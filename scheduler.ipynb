{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule --quiet"
      ],
      "metadata": {
        "id": "GZEs_Xp16Ka1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2AYSh4U5Xyj"
      },
      "outputs": [],
      "source": [
        "import schedule\n",
        "import time\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "# Set up logging for the scheduled jobs\n",
        "logging.basicConfig(filename='/content/drive/My Drive/ETL_Pipeline_Muhammad Waqas Ali_DS-019_2023-24/logs/etl_scheduler.log',\n",
        "                    level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Function to run the ETL pipeline\n",
        "def run_etl_pipeline():\n",
        "    try:\n",
        "        # Log the start time of the ETL process\n",
        "        logging.info(\"ETL process started.\")\n",
        "\n",
        "        # Running the ETL pipeline by executing the notebook\n",
        "        subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"notebook\", \"--execute\", \"/content/drive/My Drive/ETL_Pipeline_Muhammad Waqas Ali_DS-019_2023-24/etl_pipeline.ipynb\"], check=True)\n",
        "\n",
        "        # Log success\n",
        "        logging.info(\"ETL process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log any errors\n",
        "        logging.error(f\"Error in running ETL pipeline: {str(e)}\")\n",
        "\n",
        "# Function to run the load_to_db pipeline\n",
        "def run_load_to_db():\n",
        "    try:\n",
        "        # Log the start time of the load to DB process\n",
        "        logging.info(\"Load to DB process started.\")\n",
        "\n",
        "        # Running the load_to_db pipeline by executing the notebook\n",
        "        subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"notebook\", \"--execute\", \"/content/drive/My Drive/ETL_Pipeline_Muhammad Waqas Ali_DS-019_2023-24/load_to_db.ipynb\"], check=True)\n",
        "\n",
        "        # Log success\n",
        "        logging.info(\"Load to DB process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log any errors\n",
        "        logging.error(f\"Error in running load_to_db pipeline: {str(e)}\")\n",
        "\n",
        "# Schedule the ETL pipeline to run once every day at midnight\n",
        "schedule.every().day.at(\"00:00\").do(run_etl_pipeline)\n",
        "\n",
        "# Schedule the load_to_db pipeline to run once every day at 1:00 AM\n",
        "schedule.every().day.at(\"01:00\").do(run_load_to_db)\n",
        "\n",
        "# Start the scheduler loop\n",
        "while True:\n",
        "    # Run all the jobs that are scheduled\n",
        "    schedule.run_pending()\n",
        "    time.sleep(60)  # wait for 1 minute before checking the schedule again\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAASf7srGryw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}